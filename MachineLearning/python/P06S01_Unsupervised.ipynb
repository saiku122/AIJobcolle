{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeansの学習と予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタリングの学習結果を理解しやすくするため、サンプルデータの生成から始めます。<br>\n",
    "make_blobsは、各軸正規分布に従った乱数を生成してくれるデータ生成関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, labels_true = make_blobs(\n",
    "    n_samples=1000,               # サンプル数\n",
    "    centers=[[-5,-5],[0,0],[5,5]],# クラスタ重心座標設定(3クラスタを2次元で作成)\n",
    "    cluster_std=1.0,              # 乱数生成時の標準偏差\n",
    "    random_state=0)\n",
    "print('Generated data X:\\n', X[:5])\n",
    "print('Generated data y:\\n', labels_true[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横軸をX0、縦軸をX1として、生成したサンプルデータを可視化します。<br>\n",
    "クラスタリングによって期待されるのは、データがいずれのクラスタに属するかを正しく予測することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X,labels_true.reshape(len(labels_true),1)]),\n",
    "                  columns=[\"X0\",\"X1\",\"label\"])\n",
    "col = df.label.map({0:'b', 1:'g', 2:'r'})\n",
    "df.plot(x='X0', y='X1', kind='scatter', c=col, colorbar=False, figsize=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeansの実行です。KMeansではクラスタ数を設定する必要があるため3としました。<br>\n",
    "下記では学習されたクラスタ重心座標を出力しています。<br>先ほど私達が設定した値とほぼ合致することを確認して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "scl = StandardScaler()\n",
    "X_std = scl.fit_transform(X)\n",
    "kmeans.fit(X_std)\n",
    "\n",
    "print(kmeans.cluster_centers_[0], ':Centroid-0 is a cluster Green')\n",
    "print(kmeans.cluster_centers_[1], ':Centroid-1 is a cluster Blue')\n",
    "print(kmeans.cluster_centers_[2], ':Centroid-2 is a cluster Red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルを使って未知座標Xのクラスタ番号の予測をしましょう。<br>予測ロジックは単純で予測したいデータと最も近いクラスタ重心座標を持つクラスタ番号を返しているだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict([[-6,-4],[-2,0],[6,8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeansにおけるクラスタ数の決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題意識\n",
    "上記では3つの重心座標の周辺にデータを生成したため、私達はデータを3つのクラスタで学習させました。<br>しかし、実際のデータについてはこのような決定はできません。<br>それではクラスタ数をどのように決めたら良いでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### エルボー法\n",
    "このような時に役立つのが<b>エルボー法</b>です。<br>\n",
    "クラスタ数を変化させていったとき、クラスタ重心点とクラスタ所属点の間の距離の総和をプロットしたものです。<br>\n",
    "クラスタ数が少ない場合、重心に近い点もあるでしょうが多くの点は遠方の点になるでしょう。<br>\n",
    "またクラスタ数が十分に大きければその総和は0に近づくことが予想できます。<br><br>\n",
    "<b>エルボー法はこの総和の低下が急速に進む箇所と緩やかに進む箇所、それらを分離する場所でクラスタ数の候補を得ましょう</b>というものです。<br>\n",
    "さて、クラスタ重心点とクラスタ所属点の距離の総和はintertia_で取得できます。<br>\n",
    "クラスタ数1以上11未満で、距離総和の減少の様子をプロットしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エルボー図の作成\n",
    "distortions = []\n",
    "for i in range(1,11):\n",
    "    km=KMeans(n_clusters=i, random_state=0).fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), distortions, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の通り、クラスタ数が3まではクラスタ数を増やすことで重心点とサンプル間の距離が急速に減少します。<br>\n",
    "しかしクラスタを4以上にしても、もはやその低下幅は限定的です。<br>\n",
    "以上より、このデータについてはクラスタ数は3で十分ではないかと考えるわけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCANによる学習と予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に密度ベースのクラスタリングであるDBSCANの実装について学びます。<br>ここではDBSCANのご利益を確認すべくサンプルデータを変更します。<br>make_moons関数で、月型のデータサンプルを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, labels_true = make_moons(\n",
    "    n_samples=1000,\n",
    "    noise=0.05,\n",
    "    random_state=0)\n",
    "print('Generated Data:\\n', X[:5])\n",
    "print('\\nGenerated Label:\\n', labels_true[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で新しい生成サンプルデータの散布図を描いています。<br>色は本データの理想的なクラスタ分割のされ方です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack([X, labels_true.reshape(len(labels_true),1)]),\n",
    "                  columns=[\"X0\",\"X1\",\"label\"])\n",
    "col = df.label.map({0:'g', 1:'b'})\n",
    "df.plot(x='X0', y='X1', kind='scatter', c=col, colorbar=False, figsize=(6,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さてこのデータをDBSCANで学習するとどうなるでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "# 学習と予測を同時に実施\n",
    "for j in range(3):\n",
    "    ax2.plot(X[db.fit_predict(X) == j, 0],\n",
    "             X[db.fit_predict(X) == j, 1], 'o',\n",
    "             label='series ' + str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の通り、このような形状にはDBSCANがフィットしていることがわかりました。実際のデータはより高次元でしょうからシンプルにKmeansが良い、DBSCANが良いと事前に決定することは困難です。クラスタ別に実施されたモデリングや施策効果結果などを通し、実証的にアルゴリズムを選択するなどしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[確認してみよう]</b> 上記DBSCANのハイパーパラメータである`eps`を0.2から1.0に変更するとどうなるか確認してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[確認してみよう]</b> ある調査への回答データが「mental_survey.tsv」に格納されています。脳の使い方には右脳型や左脳型があるなどと言われていますが、このデータからそのような傾向は確認できるでしょうか。データカラムの`Q1`から`Q20`の20変数をKMeans法でクラスタリングし、そのクラスタ重心の解釈をしてみて下さい。カラム内容は「mental_survey_codebook.txt」で確認できます。またサンプルデータは「タブ区切り」である点を留意ください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
