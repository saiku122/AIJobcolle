{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JDLA_simple_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KfqeIp4-3Ed",
        "colab_type": "code",
        "outputId": "f80cdee4-05c0-431a-c25f-30528b6b6775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, RepeatVector\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def n(digits=3):\n",
        "  number = ''\n",
        "  for i in range(np.random.randint(1, digits + 1)):\n",
        "    number += np.random.choice(list('0123456789'))\n",
        "  return int(number)\n",
        "\n",
        "\n",
        "def padding(chars, maxlen):\n",
        "  return chars + ' ' * (maxlen - len(chars))\n",
        "\n",
        "\n",
        "'''\n",
        "データの生成\n",
        "'''\n",
        "N = 20000\n",
        "N_train = int(N * 0.9)\n",
        "N_validation = N - N_train\n",
        "\n",
        "digits = 3  # 最大の桁数\n",
        "input_digits = digits * 2 + 1  # 例： 123+456\n",
        "output_digits = digits + 1  # 500+500 = 1000 以上で４桁になる\n",
        "\n",
        "added = set()\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "while len(questions) < N:\n",
        "  a, b = n(), n()  # 適当な数を２つ生成\n",
        "\n",
        "  pair = tuple(sorted((a, b)))\n",
        "  if pair in added:\n",
        "    continue\n",
        "\n",
        "  question = '{}+{}'.format(a, b)\n",
        "  question = padding(question, input_digits)\n",
        "  answer = str(a + b)\n",
        "  answer = padding(answer, output_digits)\n",
        "\n",
        "  added.add(pair)\n",
        "  questions.append(question)\n",
        "  answers.append(answer)\n",
        "\n",
        "chars = '0123456789+ '\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
        "Y = np.zeros((len(questions), digits + 1, len(chars)), dtype=np.integer)\n",
        "\n",
        "for i in range(N):\n",
        "  for t, char in enumerate(questions[i]):\n",
        "    X[i, t, char_indices[char]] = 1\n",
        "  for t, char in enumerate(answers[i]):\n",
        "    Y[i, t, char_indices[char]] = 1\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = \\\n",
        "    train_test_split(X, Y, train_size=N_train)\n",
        "\n",
        "print(Y_train)\n",
        "\n",
        "'''\n",
        "モデル設定\n",
        "'''\n",
        "n_in = len(chars)\n",
        "n_hidden = 128\n",
        "n_out = len(chars)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Encoder\n",
        "model.add(LSTM(n_hidden, input_shape=(input_digits, n_in)))\n",
        "\n",
        "# Decoder\n",
        "model.add(RepeatVector(output_digits))\n",
        "model.add(LSTM(n_hidden, return_sequences=True))\n",
        "\n",
        "model.add(TimeDistributed(Dense(n_out)))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001,\n",
        "                             beta_1=0.9,\n",
        "                             beta_2=0.999),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "モデル学習\n",
        "'''\n",
        "epochs = 50\n",
        "batch_size = 200\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "h = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(X_validation, Y_validation))\n",
        "\n",
        "# accuracy と validation accuracy の推移をプロットする\n",
        "plt.title('Accuracy')\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.plot(np.arange(len(h.history['acc'])), h.history['acc'], label='train')\n",
        "plt.plot(np.arange(len(h.history['acc'])), h.history['val_acc'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 検証データからランダムに問題を選んで答え合わせ\n",
        "for i in range(10):\n",
        "  index = np.random.randint(0, N_validation)\n",
        "  question = X_validation[np.array([index])]\n",
        "  answer = Y_validation[np.array([index])]\n",
        "  prediction = model.predict_classes(question, verbose=0)\n",
        "\n",
        "  question = question.argmax(axis=-1)\n",
        "  answer = answer.argmax(axis=-1)\n",
        "\n",
        "  q = ''.join(indices_char[i] for i in question[0])\n",
        "  a = ''.join(indices_char[i] for i in answer[0])\n",
        "  p = ''.join(indices_char[i] for i in prediction[0])\n",
        "\n",
        "  print('-' * 10)\n",
        "  print('Q:  ', q)\n",
        "  print('A:  ', p)\n",
        "  print('T/F:', end=' ')\n",
        "  if a == p:\n",
        "    print('T')\n",
        "  else:\n",
        "    print('F')\n",
        "print('-' * 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 1]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 1]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 1 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 1]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 1 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 1]]\n",
            "\n",
            " [[0 0 0 ... 1 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 1 0 0]\n",
            "  [0 0 0 ... 0 0 1]]\n",
            "\n",
            " [[0 0 1 ... 0 0 0]\n",
            "  [1 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 1]]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 12)             0         \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(18000, 7, 12)\n",
            "(18000, 4, 12)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 18000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            "18000/18000 [==============================] - 9s 474us/step - loss: 2.0423 - acc: 0.2928 - val_loss: 1.8095 - val_acc: 0.3499\n",
            "Epoch 2/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 1.7961 - acc: 0.3499 - val_loss: 1.7760 - val_acc: 0.3584\n",
            "Epoch 3/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.7707 - acc: 0.3557 - val_loss: 1.7570 - val_acc: 0.3580\n",
            "Epoch 4/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 1.7507 - acc: 0.3588 - val_loss: 1.7320 - val_acc: 0.3625\n",
            "Epoch 5/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.7254 - acc: 0.3659 - val_loss: 1.7094 - val_acc: 0.3678\n",
            "Epoch 6/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.6919 - acc: 0.3737 - val_loss: 1.6780 - val_acc: 0.3816\n",
            "Epoch 7/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 1.6590 - acc: 0.3855 - val_loss: 1.6432 - val_acc: 0.3874\n",
            "Epoch 8/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.6181 - acc: 0.4002 - val_loss: 1.5979 - val_acc: 0.4045\n",
            "Epoch 9/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 1.5808 - acc: 0.4162 - val_loss: 1.5638 - val_acc: 0.4184\n",
            "Epoch 10/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.5505 - acc: 0.4280 - val_loss: 1.5278 - val_acc: 0.4366\n",
            "Epoch 11/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 1.5010 - acc: 0.4474 - val_loss: 1.4804 - val_acc: 0.4584\n",
            "Epoch 12/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.4483 - acc: 0.4667 - val_loss: 1.4340 - val_acc: 0.4714\n",
            "Epoch 13/50\n",
            "18000/18000 [==============================] - 3s 178us/step - loss: 1.3973 - acc: 0.4875 - val_loss: 1.3823 - val_acc: 0.4860\n",
            "Epoch 14/50\n",
            "18000/18000 [==============================] - 3s 178us/step - loss: 1.3431 - acc: 0.5064 - val_loss: 1.3399 - val_acc: 0.5038\n",
            "Epoch 15/50\n",
            "18000/18000 [==============================] - 3s 178us/step - loss: 1.2930 - acc: 0.5221 - val_loss: 1.2898 - val_acc: 0.5179\n",
            "Epoch 16/50\n",
            "18000/18000 [==============================] - 3s 179us/step - loss: 1.2556 - acc: 0.5358 - val_loss: 1.2760 - val_acc: 0.5249\n",
            "Epoch 17/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.2125 - acc: 0.5514 - val_loss: 1.2169 - val_acc: 0.5375\n",
            "Epoch 18/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 1.1713 - acc: 0.5667 - val_loss: 1.1876 - val_acc: 0.5561\n",
            "Epoch 19/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 1.1345 - acc: 0.5805 - val_loss: 1.1454 - val_acc: 0.5671\n",
            "Epoch 20/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.1068 - acc: 0.5927 - val_loss: 1.1142 - val_acc: 0.5809\n",
            "Epoch 21/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 1.0671 - acc: 0.6061 - val_loss: 1.0786 - val_acc: 0.5942\n",
            "Epoch 22/50\n",
            "18000/18000 [==============================] - 3s 178us/step - loss: 1.0293 - acc: 0.6189 - val_loss: 1.0497 - val_acc: 0.6071\n",
            "Epoch 23/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 0.9946 - acc: 0.6320 - val_loss: 1.0183 - val_acc: 0.6145\n",
            "Epoch 24/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 0.9577 - acc: 0.6444 - val_loss: 0.9790 - val_acc: 0.6245\n",
            "Epoch 25/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 0.9067 - acc: 0.6640 - val_loss: 0.9359 - val_acc: 0.6396\n",
            "Epoch 26/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 0.8543 - acc: 0.6832 - val_loss: 0.8624 - val_acc: 0.6699\n",
            "Epoch 27/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.8009 - acc: 0.7031 - val_loss: 0.8117 - val_acc: 0.6925\n",
            "Epoch 28/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.7438 - acc: 0.7260 - val_loss: 0.7568 - val_acc: 0.7170\n",
            "Epoch 29/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.6950 - acc: 0.7450 - val_loss: 0.7122 - val_acc: 0.7284\n",
            "Epoch 30/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.6364 - acc: 0.7759 - val_loss: 0.6603 - val_acc: 0.7588\n",
            "Epoch 31/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.5913 - acc: 0.7965 - val_loss: 0.6132 - val_acc: 0.7788\n",
            "Epoch 32/50\n",
            "18000/18000 [==============================] - 3s 178us/step - loss: 0.5540 - acc: 0.8138 - val_loss: 0.6018 - val_acc: 0.7783\n",
            "Epoch 33/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.5154 - acc: 0.8316 - val_loss: 0.5477 - val_acc: 0.8014\n",
            "Epoch 34/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 0.4806 - acc: 0.8467 - val_loss: 0.5127 - val_acc: 0.8228\n",
            "Epoch 35/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.4516 - acc: 0.8591 - val_loss: 0.4927 - val_acc: 0.8236\n",
            "Epoch 36/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.4251 - acc: 0.8687 - val_loss: 0.4607 - val_acc: 0.8400\n",
            "Epoch 37/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.3926 - acc: 0.8855 - val_loss: 0.4255 - val_acc: 0.8551\n",
            "Epoch 38/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.3750 - acc: 0.8902 - val_loss: 0.4248 - val_acc: 0.8558\n",
            "Epoch 39/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 0.3549 - acc: 0.8979 - val_loss: 0.3892 - val_acc: 0.8716\n",
            "Epoch 40/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 0.3309 - acc: 0.9079 - val_loss: 0.3778 - val_acc: 0.8709\n",
            "Epoch 41/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 0.3133 - acc: 0.9127 - val_loss: 0.3552 - val_acc: 0.8871\n",
            "Epoch 42/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.2968 - acc: 0.9181 - val_loss: 0.3365 - val_acc: 0.8931\n",
            "Epoch 43/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.2786 - acc: 0.9250 - val_loss: 0.3314 - val_acc: 0.8889\n",
            "Epoch 44/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.2649 - acc: 0.9290 - val_loss: 0.3165 - val_acc: 0.8941\n",
            "Epoch 45/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.2489 - acc: 0.9353 - val_loss: 0.2955 - val_acc: 0.9063\n",
            "Epoch 46/50\n",
            "18000/18000 [==============================] - 3s 174us/step - loss: 0.2385 - acc: 0.9370 - val_loss: 0.2870 - val_acc: 0.9036\n",
            "Epoch 47/50\n",
            "18000/18000 [==============================] - 3s 175us/step - loss: 0.2327 - acc: 0.9367 - val_loss: 0.2751 - val_acc: 0.9120\n",
            "Epoch 48/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.2093 - acc: 0.9479 - val_loss: 0.2658 - val_acc: 0.9149\n",
            "Epoch 49/50\n",
            "18000/18000 [==============================] - 3s 177us/step - loss: 0.1977 - acc: 0.9509 - val_loss: 0.2544 - val_acc: 0.9155\n",
            "Epoch 50/50\n",
            "18000/18000 [==============================] - 3s 176us/step - loss: 0.1891 - acc: 0.9538 - val_loss: 0.2407 - val_acc: 0.9232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "Q:   80+789 \n",
            "A:   869 \n",
            "T/F: T\n",
            "----------\n",
            "Q:   873+561\n",
            "A:   1434\n",
            "T/F: T\n",
            "----------\n",
            "Q:   107+92 \n",
            "A:   209 \n",
            "T/F: F\n",
            "----------\n",
            "Q:   779+79 \n",
            "A:   848 \n",
            "T/F: F\n",
            "----------\n",
            "Q:   9+688  \n",
            "A:   697 \n",
            "T/F: T\n",
            "----------\n",
            "Q:   507+23 \n",
            "A:   530 \n",
            "T/F: T\n",
            "----------\n",
            "Q:   582+16 \n",
            "A:   598 \n",
            "T/F: T\n",
            "----------\n",
            "Q:   17+121 \n",
            "A:   139 \n",
            "T/F: F\n",
            "----------\n",
            "Q:   329+2  \n",
            "A:   331 \n",
            "T/F: T\n",
            "----------\n",
            "Q:   405+72 \n",
            "A:   477 \n",
            "T/F: T\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}