{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_scratch3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIs5m_xmR3xk",
        "colab_type": "text"
      },
      "source": [
        "前回は勾配降下法による学習がどんなものかを紹介しましたが、ごく小規模なモデルしか考慮していませんでした。\n",
        "今回はもう少し見通しのよいコードを書きましょう。\n",
        "以下では概ね https://github.com/oreilly-japan/deep-learning-from-scratch のコードを参考にしています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_fBjELdHhCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDkF_WBbZZG7",
        "colab_type": "text"
      },
      "source": [
        "## ニューロンの定義\n",
        "\n",
        "活性化関数(Sigmoid, ReLU)と x・W+b の計算(Affine) を定義しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtfpxLHnKH2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        y = x.copy()\n",
        "        y[self.mask] = 0\n",
        "        return y\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = sigmoid(x)\n",
        "        return self.out\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        y = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return y\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JawJuZ6_Z6r7",
        "colab_type": "text"
      },
      "source": [
        "## ネットワークの定義\n",
        "\n",
        "ネットワークを定義します。\n",
        "\n",
        "パラメータを表す `params` と、ネットワークの構造を表す `layers` からなります。\n",
        "特に、 `layers` を層ごとに定義することで、あとでわかるように実装がかなり簡単になります。\n",
        "\n",
        "![代替テキスト](https://dl.dropboxusercontent.com/s/3m3xhc6j9vfllqu/nn_sample.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7tMBD3hRw-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "\n",
        "params = {}\n",
        "params['W1'] = np.array([[1, -2], [-3, 4]]) #np.random.randn(input_size, hidden_size)\n",
        "params['b1'] = np.array([1, 2]) #np.random.randn(hidden_size)\n",
        "params['W2'] = np.array([[2, 3], [4, -5]]) #np.random.randn(input_size, hidden_size)\n",
        "params['b2'] = np.array([-7, 8]) #np.random.randn(hidden_size)\n",
        "\n",
        "layers = OrderedDict()\n",
        "layers['Affine1'] = Affine(params['W1'], params['b1'])\n",
        "layers['Relu1'] = Relu()\n",
        "layers['Affine2'] = Affine(params['W2'], params['b2'])\n",
        "\n",
        "def predict(x):\n",
        "    for layer in layers.values():\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtG-sVZQad0Y",
        "colab_type": "text"
      },
      "source": [
        "## 確認\n",
        "\n",
        "このようなネットワークに入力 1,2 を入力すると 25, -32 が出力され、 -3, -4 を入力すると 13, 38 が出力することがわかります。実際にこの結果が正しいかどうか、手計算でも確認してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4LftDp2acFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3fd6edf-c176-46de-f8aa-c9907383a834"
      },
      "source": [
        "x = np.array([[1,2], [-3,-4]])\n",
        "y = predict(x)\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 25 -32]\n",
            " [ 13  38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEkMZfyxawhr",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation による勾配計算\n",
        "\n",
        "このままだと前向きの計算しかできないので、Backpropagationを使って勾配を計算できるようにしましょう。\n",
        "\n",
        "先程の実装に加えて、 `backward` というメソッドを定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUJ6P_MuwFVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx\n",
        "\n",
        "class Sigmoid:\n",
        "    #\n",
        "    # Sigmoid ではbackwardの計算のときに「forwardのときの出力」が必要です。\n",
        "    # このため self.out に forward 時の計算結果を保持しています。\n",
        "    #\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = sigmoid(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dy):\n",
        "        dx = dy * (1 - self.out) * self.out\n",
        "        return dx\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "        return dx\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mrMtTC4buv3",
        "colab_type": "text"
      },
      "source": [
        "MSEを使ったときのLossも定義しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P78qz6ZWbwdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, y, t):\n",
        "        self.t = t\n",
        "        self.y = y\n",
        "        self.loss = (0.5 * np.sum((y-t) ** 2))  / self.y.shape[0]\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.y - self.t\n",
        "        return dx / self.y.shape[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s279LqKVb4mC",
        "colab_type": "text"
      },
      "source": [
        "さきほどと同じように `params` と `layers` を定義します。\n",
        "ほとんど同じですが、初期値として正規乱数を使いました。\n",
        "また、最後にlossを計算するための層を定義しました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu3BbKTzcVQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "np.random.seed(0)\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "\n",
        "params = {}\n",
        "params['W1'] = np.random.randn(input_size, hidden_size)\n",
        "params['b1'] = np.zeros(hidden_size)\n",
        "params['W2'] = np.random.randn(input_size, hidden_size)\n",
        "params['b2'] = np.zeros(hidden_size)\n",
        "\n",
        "layers = OrderedDict()\n",
        "layers['Affine1'] = Affine(params['W1'], params['b1'])\n",
        "layers['Relu1'] = Relu()\n",
        "layers['Affine2'] = Affine(params['W2'], params['b2'])\n",
        "\n",
        "lastLayer = LinearWithLoss()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GntySJwsclA4",
        "colab_type": "text"
      },
      "source": [
        "適当な学習データを作りましょう。今回は2入力2出力のニューラルネットワークなので、入力の和と差を計算するようにしてみます。\n",
        "\n",
        "たとえば、`(0.1, 0.2)` が入力されたら `(0.3, -0.1)` が出力されるようになればうまく学習できていることになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0F1X-JRc-u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(1000):\n",
        "    x1 = np.random.rand()\n",
        "    x2 = np.random.rand()\n",
        "    y1 = x1 + x2\n",
        "    y2 = x1 - x2\n",
        "\n",
        "    x_train.append([x1, x2])\n",
        "    y_train.append([y1, y2])\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEatp_LndP4B",
        "colab_type": "text"
      },
      "source": [
        "あると便利な適当な関数をいくつか作ります。\n",
        "\n",
        "`predict` はネットワークに対して `x` を入力して、 出力 `y` を返す関数です。これまでの実装によって非常に簡潔です。\n",
        "\n",
        "`loss` はその名の通り loss を計算する関数です。\n",
        "\n",
        "`gradient` は勾配を計算する関数です。以前は数値微分によって計算していましたが、今回は Backpropagation によって計算しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIoxsmA_x1ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(x):\n",
        "    for layer in layers.values():\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "def loss(x, t):\n",
        "    y = predict(x)\n",
        "    return lastLayer.forward(y, t)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySlD-ucunZWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(layers, lastLayer):\n",
        "    dout = 1\n",
        "    dout = lastLayer.backward(dout)\n",
        "\n",
        "    for layer in list(layers.values())[::-1]:\n",
        "        dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = layers['Affine1'].dW\n",
        "    grads['b1'] = layers['Affine1'].db\n",
        "    grads['W2'] = layers['Affine2'].dW\n",
        "    grads['b2'] = layers['Affine2'].db\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jSbcBZpfLWN",
        "colab_type": "text"
      },
      "source": [
        "初期化されたまだなにも学習していない状態のニューラルネットワークに (0.1, 0.2) を入力して、出力とlossを表示してみます。\n",
        "\n",
        "出力は (1.16, -0.44) とまったく異なる結果です。ここから学習をして正しい結果が得られるようになるのかを見ていきましょう。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EGfpDNpfFH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "44bbf172-d922-4c95-8322-0d48782b9056"
      },
      "source": [
        "x = np.array([[0.1,0.2]])\n",
        "y = predict(x)\n",
        "e = loss(x_train, y_train)\n",
        "\n",
        "print('y')\n",
        "print(y)\n",
        "\n",
        "print('e')\n",
        "print(e)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y\n",
            "[[ 1.1588448  -0.43758847]]\n",
            "e\n",
            "6.264639564364666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM3QFDNFgO8p",
        "colab_type": "text"
      },
      "source": [
        "## SGD による学習\n",
        "\n",
        "すでに勾配の計算ができるようになっているので、あとはとても簡単です。\n",
        "\n",
        "- 勾配を計算する\n",
        "- パラメータ ← パラメータ - 学習率 * 勾配　としてパラメータを更新"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYxlZXasfKl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "857f2800-c006-44d5-cada-43cac9422697"
      },
      "source": [
        "lr = 0.1\n",
        "n_epochs = 1000\n",
        "for i in range(n_epochs):\n",
        "    e = loss(x_train, y_train)\n",
        "    grad = gradient(layers, lastLayer)\n",
        "\n",
        "    params['W1'] -= lr * grad['W1']\n",
        "    params['b1'] -= lr * grad['b1']\n",
        "    params['W2'] -= lr * grad['W2']\n",
        "    params['b2'] -= lr * grad['b2']\n",
        "\n",
        "    #print(i, params['W1'])\n",
        "\n",
        "    e = loss(x_train, y_train)\n",
        "    print('loss=', e)\n",
        "\n",
        "print(params['W1'])\n",
        "print(params['b1'])\n",
        "print(params['W2'])\n",
        "print(params['b2'])\n",
        "\n",
        "x = np.array([[0.1,0.2]])\n",
        "y = predict(x)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss= 0.2615180997303403\n",
            "loss= 0.2421363297173496\n",
            "loss= 0.2299152301860001\n",
            "loss= 0.22054939444843036\n",
            "loss= 0.21248881330837593\n",
            "loss= 0.20517107054889402\n",
            "loss= 0.19834481104656568\n",
            "loss= 0.1919589157799367\n",
            "loss= 0.18591784225039196\n",
            "loss= 0.18018116912282253\n",
            "loss= 0.17474625215033796\n",
            "loss= 0.16954287660624112\n",
            "loss= 0.16456352449846998\n",
            "loss= 0.15980725232799017\n",
            "loss= 0.1552528168870208\n",
            "loss= 0.1509362478535278\n",
            "loss= 0.14687667170575702\n",
            "loss= 0.1430268006657721\n",
            "loss= 0.13939517800727955\n",
            "loss= 0.13597178256619372\n",
            "loss= 0.13273543091503648\n",
            "loss= 0.1296709105972484\n",
            "loss= 0.1267778499868967\n",
            "loss= 0.12404806967773313\n",
            "loss= 0.12149209874214444\n",
            "loss= 0.11908498232138003\n",
            "loss= 0.11681465276401007\n",
            "loss= 0.11465371710257542\n",
            "loss= 0.11262331329148959\n",
            "loss= 0.11071815815587663\n",
            "loss= 0.10890558227684019\n",
            "loss= 0.10717039131465915\n",
            "loss= 0.10551337674606798\n",
            "loss= 0.10392794505280113\n",
            "loss= 0.10242813813089802\n",
            "loss= 0.10100324076877049\n",
            "loss= 0.09964629253896931\n",
            "loss= 0.09835676617019191\n",
            "loss= 0.09714020566422167\n",
            "loss= 0.09598682682498885\n",
            "loss= 0.09488727899508782\n",
            "loss= 0.09383466007166859\n",
            "loss= 0.09282884857873279\n",
            "loss= 0.09187054872877633\n",
            "loss= 0.09095597114332664\n",
            "loss= 0.09008296084565938\n",
            "loss= 0.08925180934650465\n",
            "loss= 0.08845859513198392\n",
            "loss= 0.08770076259612994\n",
            "loss= 0.08697522812422147\n",
            "loss= 0.0862796389260163\n",
            "loss= 0.08561002327525569\n",
            "loss= 0.08496483492603124\n",
            "loss= 0.08434175594773725\n",
            "loss= 0.08373865281015472\n",
            "loss= 0.08315227101123396\n",
            "loss= 0.08258404846978291\n",
            "loss= 0.08203204535446892\n",
            "loss= 0.08149463781177499\n",
            "loss= 0.08097120194050018\n",
            "loss= 0.08046017242025898\n",
            "loss= 0.0799601241683508\n",
            "loss= 0.07946990061454587\n",
            "loss= 0.07898927819235596\n",
            "loss= 0.07851723568851149\n",
            "loss= 0.07805282267466127\n",
            "loss= 0.0775952066464984\n",
            "loss= 0.07714363874126777\n",
            "loss= 0.07669733806680246\n",
            "loss= 0.07625511716638875\n",
            "loss= 0.07581697164427971\n",
            "loss= 0.07538153224352541\n",
            "loss= 0.07494915567646092\n",
            "loss= 0.07451945301969909\n",
            "loss= 0.07409207296397291\n",
            "loss= 0.0736664608127993\n",
            "loss= 0.07324102093364265\n",
            "loss= 0.07281599136710312\n",
            "loss= 0.07239047281817691\n",
            "loss= 0.07196448466178292\n",
            "loss= 0.07153891857319823\n",
            "loss= 0.07111299781061241\n",
            "loss= 0.07068565057414503\n",
            "loss= 0.07025711751786895\n",
            "loss= 0.06982778524401019\n",
            "loss= 0.06939691396991594\n",
            "loss= 0.06896368746433854\n",
            "loss= 0.06852839826655491\n",
            "loss= 0.06809110578346186\n",
            "loss= 0.06765266033508642\n",
            "loss= 0.06721265812898172\n",
            "loss= 0.0667712321978395\n",
            "loss= 0.0663279815755362\n",
            "loss= 0.06588045953165382\n",
            "loss= 0.06542940226490078\n",
            "loss= 0.06497744766098165\n",
            "loss= 0.06452248121283176\n",
            "loss= 0.06406315526190906\n",
            "loss= 0.06360172718549512\n",
            "loss= 0.06313473669662052\n",
            "loss= 0.0626626395329142\n",
            "loss= 0.062184478590440746\n",
            "loss= 0.06170312345193877\n",
            "loss= 0.06121813552255152\n",
            "loss= 0.060731133741998664\n",
            "loss= 0.0602381797929371\n",
            "loss= 0.05974013972078075\n",
            "loss= 0.05923413519482829\n",
            "loss= 0.05872199376632122\n",
            "loss= 0.05820649476415592\n",
            "loss= 0.05768496083268253\n",
            "loss= 0.057157529245182144\n",
            "loss= 0.05662484878097842\n",
            "loss= 0.05608870225373056\n",
            "loss= 0.05554622450575863\n",
            "loss= 0.05500205168055075\n",
            "loss= 0.054455969712071436\n",
            "loss= 0.05390902202940991\n",
            "loss= 0.05336190534558131\n",
            "loss= 0.05281041385083796\n",
            "loss= 0.05225284975152151\n",
            "loss= 0.051692312716699156\n",
            "loss= 0.05112948114287403\n",
            "loss= 0.05056717055078547\n",
            "loss= 0.05000516293515721\n",
            "loss= 0.04944233960342921\n",
            "loss= 0.04887801547451842\n",
            "loss= 0.048313936408934666\n",
            "loss= 0.047749253630322716\n",
            "loss= 0.047185956560987875\n",
            "loss= 0.04662507490644155\n",
            "loss= 0.04606447022267744\n",
            "loss= 0.04550395240138877\n",
            "loss= 0.04494352119821183\n",
            "loss= 0.04438237823716255\n",
            "loss= 0.04382325267896782\n",
            "loss= 0.04326381955858278\n",
            "loss= 0.042706070370262944\n",
            "loss= 0.04215038977851722\n",
            "loss= 0.041595377292978415\n",
            "loss= 0.04104219271380088\n",
            "loss= 0.040490736742823115\n",
            "loss= 0.03994031220178598\n",
            "loss= 0.03939132240542829\n",
            "loss= 0.03884210200523448\n",
            "loss= 0.03829252345345489\n",
            "loss= 0.03774345134998098\n",
            "loss= 0.03719691582504527\n",
            "loss= 0.03665304456771023\n",
            "loss= 0.036112238756122864\n",
            "loss= 0.03557595156063289\n",
            "loss= 0.03504496233799103\n",
            "loss= 0.034518072100541516\n",
            "loss= 0.03399612820659603\n",
            "loss= 0.03347816799617226\n",
            "loss= 0.032964148185928265\n",
            "loss= 0.03245417341818032\n",
            "loss= 0.03194933883011088\n",
            "loss= 0.03144974348464177\n",
            "loss= 0.030954249229445927\n",
            "loss= 0.030461324478987247\n",
            "loss= 0.029973784450969\n",
            "loss= 0.029489131336497956\n",
            "loss= 0.029011328345656714\n",
            "loss= 0.028539254609420254\n",
            "loss= 0.028073075753815572\n",
            "loss= 0.027612638782111554\n",
            "loss= 0.027159160338696985\n",
            "loss= 0.02671205179548835\n",
            "loss= 0.02626829539578626\n",
            "loss= 0.025828862101465254\n",
            "loss= 0.025394572777805515\n",
            "loss= 0.0249654766460773\n",
            "loss= 0.02454159524878373\n",
            "loss= 0.02412410427939429\n",
            "loss= 0.023711958959471247\n",
            "loss= 0.02330391417484796\n",
            "loss= 0.022901875064419576\n",
            "loss= 0.022505560853585675\n",
            "loss= 0.022114577806795623\n",
            "loss= 0.021729959139580656\n",
            "loss= 0.021351104438101298\n",
            "loss= 0.02097789586521345\n",
            "loss= 0.02060965327504768\n",
            "loss= 0.020247091682159896\n",
            "loss= 0.019889952579479602\n",
            "loss= 0.01953795378995403\n",
            "loss= 0.01919129381226357\n",
            "loss= 0.0188498956183825\n",
            "loss= 0.018512286285678796\n",
            "loss= 0.018179119275971465\n",
            "loss= 0.017851999848038404\n",
            "loss= 0.017529813320553467\n",
            "loss= 0.01721173986178166\n",
            "loss= 0.016898805809530675\n",
            "loss= 0.0165913090349\n",
            "loss= 0.016288969070869092\n",
            "loss= 0.015991595041259996\n",
            "loss= 0.0156987152773721\n",
            "loss= 0.015410777998829048\n",
            "loss= 0.015128606835600024\n",
            "loss= 0.014852334656110197\n",
            "loss= 0.014581184143653425\n",
            "loss= 0.014314643006315105\n",
            "loss= 0.014051812330058592\n",
            "loss= 0.013793794100751603\n",
            "loss= 0.01354051745674699\n",
            "loss= 0.013291706261932849\n",
            "loss= 0.013047391023253243\n",
            "loss= 0.012807944805945845\n",
            "loss= 0.012572972860152333\n",
            "loss= 0.012341875448178089\n",
            "loss= 0.012115580144590648\n",
            "loss= 0.011893930275533631\n",
            "loss= 0.011676716949066293\n",
            "loss= 0.011463351324227346\n",
            "loss= 0.011253579138405155\n",
            "loss= 0.011047272456088487\n",
            "loss= 0.010844682549447246\n",
            "loss= 0.010644993252152176\n",
            "loss= 0.010449434335997307\n",
            "loss= 0.010258193917160874\n",
            "loss= 0.010070768402616637\n",
            "loss= 0.00988727756391213\n",
            "loss= 0.009706663191925188\n",
            "loss= 0.009529750169873242\n",
            "loss= 0.009356468432910825\n",
            "loss= 0.009186664730972125\n",
            "loss= 0.009019940187757279\n",
            "loss= 0.008856621509277997\n",
            "loss= 0.00869661784533868\n",
            "loss= 0.00854003988636218\n",
            "loss= 0.008386684252297123\n",
            "loss= 0.00823670032576328\n",
            "loss= 0.008089610383482984\n",
            "loss= 0.007945362039149667\n",
            "loss= 0.007804337277279188\n",
            "loss= 0.0076664395430534734\n",
            "loss= 0.0075312623874816634\n",
            "loss= 0.00739861782462898\n",
            "loss= 0.007268724483914852\n",
            "loss= 0.007141704006660492\n",
            "loss= 0.007017333275410399\n",
            "loss= 0.006895582661408181\n",
            "loss= 0.0067759097226314175\n",
            "loss= 0.00665834679268631\n",
            "loss= 0.006543120207308565\n",
            "loss= 0.006430290982449936\n",
            "loss= 0.006319817077078357\n",
            "loss= 0.006211749073250319\n",
            "loss= 0.006105932148377274\n",
            "loss= 0.006002277976903212\n",
            "loss= 0.005900898607798661\n",
            "loss= 0.005801570185641918\n",
            "loss= 0.005704109242695952\n",
            "loss= 0.005608801120814352\n",
            "loss= 0.005515532186737094\n",
            "loss= 0.0054242051957634095\n",
            "loss= 0.005334627020237562\n",
            "loss= 0.005246682765213751\n",
            "loss= 0.005160186438238209\n",
            "loss= 0.005075385213190628\n",
            "loss= 0.004992386995805743\n",
            "loss= 0.00491086321684269\n",
            "loss= 0.004830932198810057\n",
            "loss= 0.004752254005197308\n",
            "loss= 0.00467497175051707\n",
            "loss= 0.004599263193260698\n",
            "loss= 0.004525219551550041\n",
            "loss= 0.004452835344765026\n",
            "loss= 0.004381992176663349\n",
            "loss= 0.004312561246985808\n",
            "loss= 0.004244554454545396\n",
            "loss= 0.004177813460233923\n",
            "loss= 0.004112545151249228\n",
            "loss= 0.004048550001954867\n",
            "loss= 0.003985844834479438\n",
            "loss= 0.003924393666046309\n",
            "loss= 0.0038639608523806544\n",
            "loss= 0.003804734663822634\n",
            "loss= 0.0037467023136472334\n",
            "loss= 0.0036897871447802997\n",
            "loss= 0.0036340061979281218\n",
            "loss= 0.0035794591029283717\n",
            "loss= 0.0035260680442763875\n",
            "loss= 0.0034735673396915916\n",
            "loss= 0.0034220981483380495\n",
            "loss= 0.0033715885869455694\n",
            "loss= 0.0033220120075865243\n",
            "loss= 0.003273304161270949\n",
            "loss= 0.0032254592182729613\n",
            "loss= 0.0031782605766363086\n",
            "loss= 0.0031318597352497143\n",
            "loss= 0.003086307355930962\n",
            "loss= 0.0030416521113488864\n",
            "loss= 0.002997930237761044\n",
            "loss= 0.002955055448496865\n",
            "loss= 0.0029131064104924278\n",
            "loss= 0.0028720045438804017\n",
            "loss= 0.0028316687871534504\n",
            "loss= 0.0027921239071324387\n",
            "loss= 0.0027532261847230907\n",
            "loss= 0.0027149217068348193\n",
            "loss= 0.0026772039755889156\n",
            "loss= 0.002640216121895775\n",
            "loss= 0.002604012709520907\n",
            "loss= 0.002568517551034486\n",
            "loss= 0.0025337133409039786\n",
            "loss= 0.0024995944569512598\n",
            "loss= 0.002466126942216208\n",
            "loss= 0.0024333192378436738\n",
            "loss= 0.0024011304573808173\n",
            "loss= 0.002369505907556786\n",
            "loss= 0.0023384338439418387\n",
            "loss= 0.00230798708316689\n",
            "loss= 0.0022780478113748914\n",
            "loss= 0.0022486253256993874\n",
            "loss= 0.0022198103115449\n",
            "loss= 0.002191589257119951\n",
            "loss= 0.0021639254584495454\n",
            "loss= 0.002136718587861433\n",
            "loss= 0.0021098560800434773\n",
            "loss= 0.0020834852392702667\n",
            "loss= 0.0020576502007264054\n",
            "loss= 0.002032318564323431\n",
            "loss= 0.002007357278712816\n",
            "loss= 0.001982777227903452\n",
            "loss= 0.001958640911235854\n",
            "loss= 0.0019348554354425378\n",
            "loss= 0.0019115443691400146\n",
            "loss= 0.001888652196056291\n",
            "loss= 0.001866168532921324\n",
            "loss= 0.0018440978396390537\n",
            "loss= 0.00182240311744179\n",
            "loss= 0.0018011349934387065\n",
            "loss= 0.0017802842889829663\n",
            "loss= 0.001759833038392945\n",
            "loss= 0.0017397029883955727\n",
            "loss= 0.0017199171268709997\n",
            "loss= 0.0017004007283107634\n",
            "loss= 0.0016812617261648616\n",
            "loss= 0.0016624875500866358\n",
            "loss= 0.0016439764951521343\n",
            "loss= 0.0016257703901488543\n",
            "loss= 0.0016079121696680753\n",
            "loss= 0.0015903555322948461\n",
            "loss= 0.0015730208672723128\n",
            "loss= 0.0015560140417113252\n",
            "loss= 0.001539327996888133\n",
            "loss= 0.001522956008983941\n",
            "loss= 0.0015068673864720719\n",
            "loss= 0.0014909894593838327\n",
            "loss= 0.0014753015120713838\n",
            "loss= 0.0014598972751487514\n",
            "loss= 0.0014447565086790268\n",
            "loss= 0.0014298431357000012\n",
            "loss= 0.0014152031046901989\n",
            "loss= 0.0014008307644647262\n",
            "loss= 0.001386720659353848\n",
            "loss= 0.0013728674742944156\n",
            "loss= 0.0013592634857565158\n",
            "loss= 0.001345847398623977\n",
            "loss= 0.0013326734806400414\n",
            "loss= 0.0013197368276836798\n",
            "loss= 0.0013070185754647272\n",
            "loss= 0.0012944198147942197\n",
            "loss= 0.0012820325171001437\n",
            "loss= 0.00126981447559465\n",
            "loss= 0.0012578118584408002\n",
            "loss= 0.0012460202935729017\n",
            "loss= 0.0012344168249613819\n",
            "loss= 0.0012229811800997337\n",
            "loss= 0.0012117447972286432\n",
            "loss= 0.0012006782580296304\n",
            "loss= 0.0011897722548916353\n",
            "loss= 0.001179054313337759\n",
            "loss= 0.0011685024000360664\n",
            "loss= 0.0011580949262252217\n",
            "loss= 0.0011478650008860261\n",
            "loss= 0.0011378091458949173\n",
            "loss= 0.0011279239923384697\n",
            "loss= 0.0011182059921482177\n",
            "loss= 0.0011086385649758874\n",
            "loss= 0.001099183162161756\n",
            "loss= 0.0010898503653743025\n",
            "loss= 0.0010806731305130295\n",
            "loss= 0.0010716314140548147\n",
            "loss= 0.0010626868161709509\n",
            "loss= 0.0010538155007736385\n",
            "loss= 0.0010450890194473412\n",
            "loss= 0.001036482843169271\n",
            "loss= 0.00102798915534368\n",
            "loss= 0.0010196332190312337\n",
            "loss= 0.0010114113603775052\n",
            "loss= 0.0010033210789481518\n",
            "loss= 0.0009953599405180076\n",
            "loss= 0.0009875113734480762\n",
            "loss= 0.0009797566198179268\n",
            "loss= 0.0009721241707311203\n",
            "loss= 0.0009646114054077177\n",
            "loss= 0.0009571733098271799\n",
            "loss= 0.0009498268238105652\n",
            "loss= 0.000942538770905977\n",
            "loss= 0.0009353476368855441\n",
            "loss= 0.0009282350662189388\n",
            "loss= 0.0009211981915248351\n",
            "loss= 0.0009142678537550802\n",
            "loss= 0.0009074420745336366\n",
            "loss= 0.0009007189773607783\n",
            "loss= 0.0008940967421291032\n",
            "loss= 0.0008875660250848411\n",
            "loss= 0.0008811016893750116\n",
            "loss= 0.0008746968297067357\n",
            "loss= 0.0008683862347716884\n",
            "loss= 0.0008621682114515472\n",
            "loss= 0.0008560291571832771\n",
            "loss= 0.0008499568930362744\n",
            "loss= 0.0008439726529006346\n",
            "loss= 0.0008380749022597703\n",
            "loss= 0.0008322621547823457\n",
            "loss= 0.0008265329588792967\n",
            "loss= 0.0008208800264414742\n",
            "loss= 0.0008152785750719303\n",
            "loss= 0.0008097565291167068\n",
            "loss= 0.0008043125304340425\n",
            "loss= 0.0007989280217304129\n",
            "loss= 0.0007935708843164557\n",
            "loss= 0.0007882746119923834\n",
            "loss= 0.000783022340421312\n",
            "loss= 0.0007778209885236517\n",
            "loss= 0.0007726905266054473\n",
            "loss= 0.0007676210099479363\n",
            "loss= 0.0007625975857619426\n",
            "loss= 0.0007576415505747031\n",
            "loss= 0.0007527517855773884\n",
            "loss= 0.000747927216456811\n",
            "loss= 0.0007431470922147727\n",
            "loss= 0.0007383605157900165\n",
            "loss= 0.0007336363129351795\n",
            "loss= 0.0007289733288640559\n",
            "loss= 0.0007243678150982011\n",
            "loss= 0.0007197964291153816\n",
            "loss= 0.0007152834260449445\n",
            "loss= 0.0007108278730719404\n",
            "loss= 0.0007064288736321689\n",
            "loss= 0.000702083232652376\n",
            "loss= 0.000697760842856512\n",
            "loss= 0.0006934663119931237\n",
            "loss= 0.0006892056317395821\n",
            "loss= 0.0006849970964039563\n",
            "loss= 0.0006808391106416774\n",
            "loss= 0.0006767256388623675\n",
            "loss= 0.0006726421568589887\n",
            "loss= 0.0006686079742302765\n",
            "loss= 0.0006646207558370343\n",
            "loss= 0.0006606578021769756\n",
            "loss= 0.0006567417663201867\n",
            "loss= 0.000652871692416645\n",
            "loss= 0.0006490468717958593\n",
            "loss= 0.000645267026619094\n",
            "loss= 0.0006415253690944376\n",
            "loss= 0.000637805936316163\n",
            "loss= 0.0006341099939706024\n",
            "loss= 0.0006304360188936773\n",
            "loss= 0.0006268039433136301\n",
            "loss= 0.0006232073266710092\n",
            "loss= 0.0006196337994858949\n",
            "loss= 0.000616100303293521\n",
            "loss= 0.0006126062556181405\n",
            "loss= 0.0006091511006758295\n",
            "loss= 0.0006057342973004889\n",
            "loss= 0.0006023553153045756\n",
            "loss= 0.0005990136342888685\n",
            "loss= 0.0005957087431617399\n",
            "loss= 0.0005924398292900958\n",
            "loss= 0.0005892040260780647\n",
            "loss= 0.0005859854264778901\n",
            "loss= 0.0005828016309550263\n",
            "loss= 0.0005796485453650031\n",
            "loss= 0.0005765136682844288\n",
            "loss= 0.0005734121995785102\n",
            "loss= 0.0005703436831083142\n",
            "loss= 0.000567298249042604\n",
            "loss= 0.0005642751920443662\n",
            "loss= 0.0005612837723675529\n",
            "loss= 0.0005583142508434907\n",
            "loss= 0.0005553656256575999\n",
            "loss= 0.000552447230224743\n",
            "loss= 0.0005495393929810362\n",
            "loss= 0.000546644883567692\n",
            "loss= 0.0005437795031654884\n",
            "loss= 0.0005409428543443056\n",
            "loss= 0.0005381264658016491\n",
            "loss= 0.0005353251550346981\n",
            "loss= 0.0005325386936032743\n",
            "loss= 0.0005297795126124995\n",
            "loss= 0.0005270468361147734\n",
            "loss= 0.0005243216812180584\n",
            "loss= 0.0005216088729177162\n",
            "loss= 0.0005189219210952893\n",
            "loss= 0.0005162604875141969\n",
            "loss= 0.0005136242556812083\n",
            "loss= 0.0005110129190384105\n",
            "loss= 0.0005084261774880187\n",
            "loss= 0.0005058601461031858\n",
            "loss= 0.0005033059057946728\n",
            "loss= 0.0005007753314203845\n",
            "loss= 0.0004982681351511394\n",
            "loss= 0.0004957840399823589\n",
            "loss= 0.0004933227751753168\n",
            "loss= 0.000490884074890426\n",
            "loss= 0.0004884676777339713\n",
            "loss= 0.00048607332656564443\n",
            "loss= 0.00048369654917386517\n",
            "loss= 0.0004813314892241906\n",
            "loss= 0.00047898769155865176\n",
            "loss= 0.00047666490864989894\n",
            "loss= 0.00047436290154994774\n",
            "loss= 0.00047208143639849054\n",
            "loss= 0.00046981749315806184\n",
            "loss= 0.0004675626575823971\n",
            "loss= 0.0004653229125995787\n",
            "loss= 0.0004630801183703672\n",
            "loss= 0.0004608565718487522\n",
            "loss= 0.0004586494952274813\n",
            "loss= 0.0004564501717996991\n",
            "loss= 0.0004542693482299478\n",
            "loss= 0.00045210681295302095\n",
            "loss= 0.0004499623670190307\n",
            "loss= 0.0004478358172034633\n",
            "loss= 0.0004457269740034332\n",
            "loss= 0.0004436356510265198\n",
            "loss= 0.00044156037947265384\n",
            "loss= 0.00043949105957048637\n",
            "loss= 0.0004374386373778106\n",
            "loss= 0.0004354029320959911\n",
            "loss= 0.00043338377032599355\n",
            "loss= 0.00043138098257675995\n",
            "loss= 0.00042939439594364684\n",
            "loss= 0.0004274208673652337\n",
            "loss= 0.0004254538438835831\n",
            "loss= 0.00042350247202773057\n",
            "loss= 0.0004215665902543956\n",
            "loss= 0.0004196460430467769\n",
            "loss= 0.00041774067819005267\n",
            "loss= 0.00041584748829768734\n",
            "loss= 0.0004139610904582388\n",
            "loss= 0.0004120893675738412\n",
            "loss= 0.0004102321717107901\n",
            "loss= 0.00040838936081560764\n",
            "loss= 0.0004065607959117509\n",
            "loss= 0.00040474491883284446\n",
            "loss= 0.00040293439014149336\n",
            "loss= 0.0004011376523487283\n",
            "loss= 0.000399354570209497\n",
            "loss= 0.0003975850135855848\n",
            "loss= 0.0003958288550547155\n",
            "loss= 0.00039408587729766254\n",
            "loss= 0.0003923555447224935\n",
            "loss= 0.00039063822768407077\n",
            "loss= 0.0003889308239636721\n",
            "loss= 0.0003872285352672415\n",
            "loss= 0.00038553838096278074\n",
            "loss= 0.000383860653359533\n",
            "loss= 0.0003821952390651385\n",
            "loss= 0.0003805420272215093\n",
            "loss= 0.00037890090872760574\n",
            "loss= 0.00037727177599533167\n",
            "loss= 0.0003756545228592533\n",
            "loss= 0.00037404904453124663\n",
            "loss= 0.00037245517839009935\n",
            "loss= 0.0003708724358664841\n",
            "loss= 0.00036930115271550376\n",
            "loss= 0.0003677412294536681\n",
            "loss= 0.00036619256785791867\n",
            "loss= 0.00036465507093058267\n",
            "loss= 0.00036312864287160875\n",
            "loss= 0.00036161318905416083\n",
            "loss= 0.00036010745449959256\n",
            "loss= 0.0003586048879880091\n",
            "loss= 0.0003571129322126477\n",
            "loss= 0.00035563149380462834\n",
            "loss= 0.000354160483708157\n",
            "loss= 0.0003526998148209544\n",
            "loss= 0.00035124845628789817\n",
            "loss= 0.0003498002982659435\n",
            "loss= 0.0003483621640423585\n",
            "loss= 0.0003469339679447385\n",
            "loss= 0.00034551562789551283\n",
            "loss= 0.0003441070395011195\n",
            "loss= 0.00034270757070916426\n",
            "loss= 0.0003413153300494245\n",
            "loss= 0.0003399275464015592\n",
            "loss= 0.0003385491596475358\n",
            "loss= 0.0003371800918096175\n",
            "loss= 0.00033582026797567894\n",
            "loss= 0.00033446961469705015\n",
            "loss= 0.0003331280595282815\n",
            "loss= 0.0003317955308879051\n",
            "loss= 0.0003304719580093708\n",
            "loss= 0.00032915727091733604\n",
            "loss= 0.0003278514004111637\n",
            "loss= 0.0003265542780505886\n",
            "loss= 0.00032526583614216775\n",
            "loss= 0.00032398600772614894\n",
            "loss= 0.0003227132616979791\n",
            "loss= 0.0003214434884487597\n",
            "loss= 0.00032018206407874667\n",
            "loss= 0.0003189289221606316\n",
            "loss= 0.0003176839992027978\n",
            "loss= 0.0003164472330061652\n",
            "loss= 0.0003152185621982252\n",
            "loss= 0.00031399792609603736\n",
            "loss= 0.00031278526466088874\n",
            "loss= 0.0003115805184784166\n",
            "loss= 0.0003103836287458173\n",
            "loss= 0.0003091945372610878\n",
            "loss= 0.0003080131864129453\n",
            "loss= 0.0003068395191710634\n",
            "loss= 0.00030567347907654586\n",
            "loss= 0.0003045150102326181\n",
            "loss= 0.0003033640572955279\n",
            "loss= 0.0003022205654656617\n",
            "loss= 0.00030108448047886223\n",
            "loss= 0.00029995574859795237\n",
            "loss= 0.00029883431660445716\n",
            "loss= 0.0002977201317905158\n",
            "loss= 0.0002966131419509854\n",
            "loss= 0.0002955131878478771\n",
            "loss= 0.0002944143169696046\n",
            "loss= 0.000293321718933369\n",
            "loss= 0.0002922311077332752\n",
            "loss= 0.00029114731470985595\n",
            "loss= 0.0002900702881068004\n",
            "loss= 0.00028899997939917354\n",
            "loss= 0.0002879363412862474\n",
            "loss= 0.00028687932712698395\n",
            "loss= 0.00028582875837531704\n",
            "loss= 0.0002847845731760685\n",
            "loss= 0.00028374686877481704\n",
            "loss= 0.0002827153891214753\n",
            "loss= 0.00028168492384812445\n",
            "loss= 0.0002806607380185967\n",
            "loss= 0.0002796427862535155\n",
            "loss= 0.0002786310255178838\n",
            "loss= 0.0002776254137032876\n",
            "loss= 0.00027662590923010246\n",
            "loss= 0.00027563247093335437\n",
            "loss= 0.0002746450416784527\n",
            "loss= 0.0002736633348273287\n",
            "loss= 0.00027268756685257034\n",
            "loss= 0.000271717698080689\n",
            "loss= 0.0002707536891940221\n",
            "loss= 0.0002697955012204075\n",
            "loss= 0.00026884309552601104\n",
            "loss= 0.00026789643380955176\n",
            "loss= 0.0002669554780971872\n",
            "loss= 0.00026602019073773545\n",
            "loss= 0.00026509053439810004\n",
            "loss= 0.0002641664720588329\n",
            "loss= 0.0002632479670098101\n",
            "loss= 0.00026233498284600844\n",
            "loss= 0.0002614274834633722\n",
            "loss= 0.00026052543305477336\n",
            "loss= 0.00025962879610605605\n",
            "loss= 0.00025873753739216754\n",
            "loss= 0.00025785162197337154\n",
            "loss= 0.0002569710151915415\n",
            "loss= 0.00025609568266653395\n",
            "loss= 0.00025522548927632403\n",
            "loss= 0.00025435590300128166\n",
            "loss= 0.00025349142487020266\n",
            "loss= 0.00025263202010073155\n",
            "loss= 0.0002517769123040602\n",
            "loss= 0.00025092294796830405\n",
            "loss= 0.00025007389761427177\n",
            "loss= 0.0002492297280643133\n",
            "loss= 0.0002483904082750418\n",
            "loss= 0.0002475559079626213\n",
            "loss= 0.000246726197221098\n",
            "loss= 0.00024590124641520316\n",
            "loss= 0.0002450810261489146\n",
            "loss= 0.0002442655072548767\n",
            "loss= 0.00024345466078954647\n",
            "loss= 0.0002426484580299133\n",
            "loss= 0.00024184687047066754\n",
            "loss= 0.00024104986982150943\n",
            "loss= 0.00024025742800453406\n",
            "loss= 0.00023946951715166225\n",
            "loss= 0.00023868610960212363\n",
            "loss= 0.00023790717789998745\n",
            "loss= 0.00023713269479173893\n",
            "loss= 0.00023636263322390587\n",
            "loss= 0.00023559696634073085\n",
            "loss= 0.00023483566206493808\n",
            "loss= 0.00023407476407640167\n",
            "loss= 0.00023331812747398915\n",
            "loss= 0.00023256572502432086\n",
            "loss= 0.0002318175310030574\n",
            "loss= 0.00023107352023133977\n",
            "loss= 0.0002303336678094746\n",
            "loss= 0.0002295979490425254\n",
            "loss= 0.00022886633941862104\n",
            "loss= 0.0002281388146016985\n",
            "loss= 0.00022741535042818272\n",
            "loss= 0.0002266959229047351\n",
            "loss= 0.0002259805082063004\n",
            "loss= 0.00022526908267425183\n",
            "loss= 0.0002245616228145749\n",
            "loss= 0.00022385683171902618\n",
            "loss= 0.000223153631945037\n",
            "loss= 0.0002224542814261834\n",
            "loss= 0.00022175875639251436\n",
            "loss= 0.00022106703429837274\n",
            "loss= 0.00022037909304376807\n",
            "loss= 0.00021969491075970616\n",
            "loss= 0.00021901446574835033\n",
            "loss= 0.00021833773646561002\n",
            "loss= 0.0002176647015153227\n",
            "loss= 0.00021699527057590407\n",
            "loss= 0.00021632911011151045\n",
            "loss= 0.00021566657534832445\n",
            "loss= 0.00021500764547231722\n",
            "loss= 0.00021435229983599795\n",
            "loss= 0.00021370051794063446\n",
            "loss= 0.00021305227942803806\n",
            "loss= 0.00021240756407628835\n",
            "loss= 0.00021176513900733122\n",
            "loss= 0.00021112403281864494\n",
            "loss= 0.0002104863393823586\n",
            "loss= 0.0002098507846634102\n",
            "loss= 0.00020921656361513937\n",
            "loss= 0.00020858564897418492\n",
            "loss= 0.00020795726438024143\n",
            "loss= 0.0002073296467560373\n",
            "loss= 0.00020670522937197772\n",
            "loss= 0.00020608399296088086\n",
            "loss= 0.00020546591986005125\n",
            "loss= 0.00020485099292667222\n",
            "loss= 0.00020423919524028146\n",
            "loss= 0.00020363051002060168\n",
            "loss= 0.00020302492060427128\n",
            "loss= 0.0002024224104376741\n",
            "loss= 0.00020182296307416433\n",
            "loss= 0.00020122649539657\n",
            "loss= 0.0002006329437321698\n",
            "loss= 0.00020004240175466837\n",
            "loss= 0.00019945485345849672\n",
            "loss= 0.00019887028294406233\n",
            "loss= 0.00019828867441359634\n",
            "loss= 0.00019771001216883688\n",
            "loss= 0.00019713428060947664\n",
            "loss= 0.00019656146423193937\n",
            "loss= 0.0001959915476283089\n",
            "loss= 0.0001954245154853337\n",
            "loss= 0.00019486035258347006\n",
            "loss= 0.00019429904379595565\n",
            "loss= 0.0001937405740879023\n",
            "loss= 0.00019318492851540752\n",
            "loss= 0.00019263209222468314\n",
            "loss= 0.00019208205045119996\n",
            "loss= 0.00019153478851884827\n",
            "loss= 0.00019099029183911387\n",
            "loss= 0.00019044854591026924\n",
            "loss= 0.00018990953631657837\n",
            "loss= 0.00018937324872751813\n",
            "loss= 0.00018883966889701127\n",
            "loss= 0.00018830878266267457\n",
            "loss= 0.00018778057594507927\n",
            "loss= 0.00018725503474702548\n",
            "loss= 0.00018673214515282794\n",
            "loss= 0.00018621189332761507\n",
            "loss= 0.00018569426551663963\n",
            "loss= 0.00018517923476414483\n",
            "loss= 0.0001846663608548266\n",
            "loss= 0.00018415370346938154\n",
            "loss= 0.00018364120891899055\n",
            "loss= 0.0001831311854117568\n",
            "loss= 0.00018262361788804574\n",
            "loss= 0.00018211849333154867\n",
            "loss= 0.00018161579933333212\n",
            "loss= 0.00018111552370081053\n",
            "loss= 0.00018061765435091253\n",
            "loss= 0.00018012217928057095\n",
            "loss= 0.0001796290865582541\n",
            "loss= 0.0001791371618844121\n",
            "loss= 0.00017864632354600067\n",
            "loss= 0.00017815779323327114\n",
            "loss= 0.00017767155864943524\n",
            "loss= 0.00017718753096065878\n",
            "loss= 0.00017670568482720434\n",
            "loss= 0.00017622609679655079\n",
            "loss= 0.0001757487558764791\n",
            "loss= 0.00017527365114418074\n",
            "loss= 0.00017480077174129505\n",
            "loss= 0.0001743301068715579\n",
            "loss= 0.0001738616457995013\n",
            "loss= 0.0001733953778495903\n",
            "loss= 0.00017293129240554745\n",
            "loss= 0.0001724693789097642\n",
            "loss= 0.00017200962686275128\n",
            "loss= 0.00017155202582261458\n",
            "loss= 0.0001710965654045412\n",
            "loss= 0.00017064323528030067\n",
            "loss= 0.0001701920251777538\n",
            "loss= 0.00016974292488037063\n",
            "loss= 0.00016929592422675722\n",
            "loss= 0.00016885101311019191\n",
            "loss= 0.00016840818147816788\n",
            "loss= 0.0001679674193319459\n",
            "loss= 0.0001675287167261126\n",
            "loss= 0.00016709206376814921\n",
            "loss= 0.00016665745061800543\n",
            "loss= 0.00016622486748768184\n",
            "loss= 0.0001657938137390537\n",
            "loss= 0.00016536311670066706\n",
            "loss= 0.00016493438328111246\n",
            "loss= 0.0001645076032980079\n",
            "loss= 0.0001640827673370892\n",
            "loss= 0.00016365963390063237\n",
            "loss= 0.00016323660412218742\n",
            "loss= 0.0001628154570997844\n",
            "loss= 0.00016239618324319138\n",
            "loss= 0.00016197877370721222\n",
            "loss= 0.00016156321988045058\n",
            "loss= 0.00016114951324715514\n",
            "loss= 0.00016073764534971154\n",
            "loss= 0.00016032760777827984\n",
            "loss= 0.00015991919767811326\n",
            "loss= 0.00015951084120116566\n",
            "loss= 0.00015910425839481734\n",
            "loss= 0.00015869944050224028\n",
            "loss= 0.00015829637939627467\n",
            "loss= 0.00015789506714940354\n",
            "loss= 0.00015749549591775636\n",
            "loss= 0.00015709765790965666\n",
            "loss= 0.00015670154537692047\n",
            "loss= 0.00015630715061226746\n",
            "loss= 0.00015591446594837076\n",
            "loss= 0.00015552348375734792\n",
            "loss= 0.00015513419645036945\n",
            "loss= 0.00015474659647730137\n",
            "loss= 0.0001543606763263624\n",
            "loss= 0.0001539764285237873\n",
            "loss= 0.0001535938456334966\n",
            "loss= 0.00015321292025677457\n",
            "loss= 0.00015283364503195003\n",
            "loss= 0.00015245601263408652\n",
            "loss= 0.00015208001577467662\n",
            "loss= 0.00015170564720134313\n",
            "loss= 0.00015133289969754504\n",
            "loss= 0.00015096176608229018\n",
            "loss= 0.0001505922392098527\n",
            "loss= 0.00015022431196949602\n",
            "loss= 0.0001498579772852005\n",
            "loss= 0.0001494932281153971\n",
            "loss= 0.00014913005745270452\n",
            "loss= 0.00014876845832367266\n",
            "loss= 0.0001484083421792608\n",
            "loss= 0.00014804815355205073\n",
            "loss= 0.00014768944361207793\n",
            "loss= 0.00014733224632149471\n",
            "loss= 0.00014697655496626112\n",
            "loss= 0.0001466223629900416\n",
            "loss= 0.00014626966390242235\n",
            "loss= 0.00014591845125381536\n",
            "loss= 0.00014556871862842762\n",
            "loss= 0.0001452204596421468\n",
            "loss= 0.00014487366794177714\n",
            "loss= 0.00014452833720465097\n",
            "loss= 0.00014418446113834848\n",
            "loss= 0.0001438420334804486\n",
            "loss= 0.000143501047998295\n",
            "loss= 0.00014316149848876534\n",
            "loss= 0.00014282337877804716\n",
            "loss= 0.0001424866827214174\n",
            "loss= 0.00014215140420302508\n",
            "loss= 0.0001418175371356796\n",
            "loss= 0.00014148507546064135\n",
            "loss= 0.000141154013147417\n",
            "loss= 0.0001408240978614579\n",
            "loss= 0.00014049425941599402\n",
            "loss= 0.00014016577573505436\n",
            "loss= 0.00013983864050225614\n",
            "loss= 0.00013951284788817772\n",
            "loss= 0.00013918839221303104\n",
            "loss= 0.00013886526785629458\n",
            "loss= 0.00013854346923241562\n",
            "loss= 0.0001382229907841796\n",
            "loss= 0.00013790382698080294\n",
            "loss= 0.00013758576115287932\n",
            "loss= 0.00013726770918261863\n",
            "loss= 0.0001369509287412685\n",
            "loss= 0.0001366354139964382\n",
            "loss= 0.00013632115959981723\n",
            "loss= 0.00013600816035009176\n",
            "loss= 0.00013569639347513755\n",
            "loss= 0.00013538568332228896\n",
            "loss= 0.00013507621016636392\n",
            "loss= 0.0001347679689473421\n",
            "loss= 0.00013446095464026816\n",
            "loss= 0.00013415483704118993\n",
            "loss= 0.0001338488178391148\n",
            "loss= 0.00013354398458434092\n",
            "loss= 0.0001332401013978848\n",
            "loss= 0.0001329362184764109\n",
            "loss= 0.00013263348301413526\n",
            "loss= 0.00013233188982877787\n",
            "loss= 0.00013203143427114998\n",
            "loss= 0.0001317317795156653\n",
            "loss= 0.00013143217079695824\n",
            "loss= 0.00013113365946722388\n",
            "loss= 0.00013083624069090742\n",
            "loss= 0.0001305399101356824\n",
            "loss= 0.00013024466361837987\n",
            "loss= 0.00012995049701048606\n",
            "loss= 0.00012965740621277984\n",
            "loss= 0.00012936538714839645\n",
            "loss= 0.00012907443576080593\n",
            "loss= 0.0001287845480131043\n",
            "loss= 0.00012849571988765996\n",
            "loss= 0.00012820794738585661\n",
            "loss= 0.0001279211126050304\n",
            "loss= 0.0001276351924670333\n",
            "loss= 0.00012735031340623914\n",
            "loss= 0.00012706647148876\n",
            "loss= 0.00012678366281512204\n",
            "loss= 0.00012650188351041058\n",
            "loss= 0.000126221129720134\n",
            "loss= 0.00012594139760838816\n",
            "loss= 0.0001256626833569758\n",
            "loss= 0.00012538493788014988\n",
            "loss= 0.000125108083898541\n",
            "loss= 0.00012483223387227751\n",
            "loss= 0.0001245573840517734\n",
            "loss= 0.0001242835307124912\n",
            "loss= 0.00012401067014970868\n",
            "loss= 0.00012373879867629497\n",
            "loss= 0.00012346791262169782\n",
            "loss= 0.0001231980083314321\n",
            "loss= 0.00012292908216678464\n",
            "loss= 0.00012266113050461087\n",
            "loss= 0.0001223941497371743\n",
            "loss= 0.00012212813627200485\n",
            "loss= 0.00012186308653176726\n",
            "loss= 0.00012159899695413487\n",
            "loss= 0.0001213358639916675\n",
            "loss= 0.00012107368411169263\n",
            "loss= 0.00012081245379618846\n",
            "loss= 0.00012055216954167056\n",
            "loss= 0.00012029282785908064\n",
            "loss= 0.00012003442527367718\n",
            "loss= 0.00011977695832492902\n",
            "loss= 0.00011952042356641104\n",
            "loss= 0.00011926481756570184\n",
            "loss= 0.00011901013690428309\n",
            "loss= 0.00011875637817744277\n",
            "loss= 0.00011850353799417735\n",
            "loss= 0.00011825161297709903\n",
            "loss= 0.00011800059976234261\n",
            "loss= 0.0001177504949994755\n",
            "loss= 0.000117501295351408\n",
            "loss= 0.0001172526363693104\n",
            "loss= 0.00011700408053777386\n",
            "loss= 0.00011675639661868816\n",
            "loss= 0.00011650958101220023\n",
            "loss= 0.00011626363049272424\n",
            "loss= 0.00011601854194279515\n",
            "loss= 0.00011577431228256326\n",
            "loss= 0.0001155309384510713\n",
            "loss= 0.0001152884174012383\n",
            "loss= 0.0001150467460984746\n",
            "loss= 0.00011480544764339304\n",
            "loss= 0.00011456434349487592\n",
            "loss= 0.00011432405740885245\n",
            "loss= 0.00011408458609487919\n",
            "loss= 0.00011384592661602803\n",
            "loss= 0.0001136080761370423\n",
            "loss= 0.00011337103185771545\n",
            "loss= 0.00011313479099521947\n",
            "loss= 0.0001128993507793711\n",
            "loss= 0.0001126647084513211\n",
            "loss= 0.00011243086126314824\n",
            "loss= 0.00011219780647769219\n",
            "loss= 0.00011196554136845023\n",
            "loss= 0.0001117340632194923\n",
            "loss= 0.00011150336932538036\n",
            "loss= 0.00011127345699109051\n",
            "loss= 0.00011104432353193746\n",
            "loss= 0.00011081596627349915\n",
            "loss= 0.0001105883825515438\n",
            "loss= 0.00011036156971195864\n",
            "loss= 0.0001101355251106791\n",
            "loss= 0.00010991024611361981\n",
            "loss= 0.00010968573009660754\n",
            "loss= 0.0001094619744453149\n",
            "loss= 0.00010923897655519496\n",
            "loss= 0.00010901673383141852\n",
            "loss= 0.00010879524368881071\n",
            "loss= 0.00010857450355179063\n",
            "[[ 1.15242774 -0.26215634]\n",
            " [ 0.43081459  2.12942655]]\n",
            "[-0.16568414  0.10721162]\n",
            "[[ 0.93468145  0.7320465 ]\n",
            " [ 0.28198257 -0.61752769]]\n",
            "[0.12060122 0.18428925]\n",
            "[[0.1 0.2]]\n",
            "[[ 0.29692118 -0.10257415]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvHQN1N_jroh",
        "colab_type": "text"
      },
      "source": [
        "# MNISTを学習する\n",
        "\n",
        "より大規模な問題も学習可能かどうか確認しましょう。ここでは手書き文字認識データセットのMNISTを使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlrO8JedjrLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a24f4541-0e49-4afd-e992-3a01504dff36"
      },
      "source": [
        "def load_mnist():\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "x_train, y_train = load_mnist()\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 784) (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_6X3Yjo9yra",
        "colab_type": "text"
      },
      "source": [
        "xが0〜255の値をとるので0〜1に正規化します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elbLYgkW9yHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype(np.float) / 255"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni1b7zG-9pt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "872edfc6-cacb-4c1b-cfda-9be1f35f121a"
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dEF1VlmnMtb",
        "colab_type": "text"
      },
      "source": [
        "`y` が one-hot ベクトルではないので one-hot にします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb8ESjjNnKxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d85ba29-2ae4-45b3-c805-3eec171b9354"
      },
      "source": [
        "\n",
        "def to_one_hot(y, n_categories):\n",
        "    if y.ndim > 1 and y.shape[1] == n_categories:\n",
        "        return y\n",
        "\n",
        "    onehot = np.zeros((y.shape[0], n_categories))\n",
        "    for i, x in enumerate(onehot):\n",
        "        onehot[i, int(y[i])] = 1\n",
        "    return onehot\n",
        "\n",
        "y_train = to_one_hot(y_train, 10)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 784) (70000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvB6FPUICI4t",
        "colab_type": "text"
      },
      "source": [
        "まだ softmax を定義していなかったので、softmax, crossentropy を定義します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byFFnTLFGVqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70ddb35b-5375-4792-80ee-ec7f421c2b3e"
      },
      "source": [
        "y =np.array([[-1, -2, -3], [-4, -5, -6], [-7, -8, -9]])\n",
        "t = [1, 0, 2]\n",
        "y[np.arange(3), t]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2, -4, -9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJrD8CKjov1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def softmax(x):\n",
        "    # 素朴な実装としては \n",
        "    # np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True) になりますが、\n",
        "    # 入力が大きくなるとオーバーフローするため、\n",
        "    # 入力から最大値を引くというテクニックを使います\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    t = t.argmax(axis=1)\n",
        "    batch_size = y.shape[0]\n",
        "    # y[np.arange(batch_size), t] は 「yのうちtが1になっているものだけを取り出す」処理です\n",
        "    # たとえば\n",
        "    #   y =np.array([[-1, -2, -3], [-4, -5, -6], [-7, -8, -9]])\n",
        "    #   t = [1, 0, 2]\n",
        "    #   y[np.arange(3), t]\n",
        "    # は [-2, -4, -9] と表示します。\n",
        "    # \n",
        "    # ただし、この結果が 0 だとゼロ除算になるため、\n",
        "    # これを回避するために小さい定数を足します\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.y.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "        return dx"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wp0nUtLnq5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 32\n",
        "output_size = 10\n",
        "\n",
        "params = {}\n",
        "params['W1'] = np.random.randn(input_size, hidden_size)\n",
        "params['b1'] = np.random.randn(hidden_size)\n",
        "params['W2'] = np.random.randn(hidden_size, hidden_size)\n",
        "params['b2'] = np.random.randn(hidden_size)\n",
        "params['W3'] = np.random.randn(hidden_size, output_size)\n",
        "params['b3'] = np.random.randn(output_size)\n",
        "\n",
        "layers = OrderedDict()\n",
        "layers['Affine1'] = Affine(params['W1'], params['b1'])\n",
        "layers['Relu1'] = Relu()\n",
        "layers['Affine2'] = Affine(params['W2'], params['b2'])\n",
        "layers['Relu2'] = Relu()\n",
        "layers['Affine3'] = Affine(params['W3'], params['b3'])\n",
        "\n",
        "lastLayer = SoftmaxWithLoss()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYZjLfg4mSWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(layers, lastLayer):\n",
        "    dout = 1\n",
        "    dout = lastLayer.backward(dout)\n",
        "\n",
        "    for layer in list(layers.values())[::-1]:\n",
        "        dout = layer.backward(dout)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = layers['Affine1'].dW\n",
        "    grads['b1'] = layers['Affine1'].db\n",
        "    grads['W2'] = layers['Affine2'].dW\n",
        "    grads['b2'] = layers['Affine2'].db\n",
        "    grads['W3'] = layers['Affine3'].dW\n",
        "    grads['b3'] = layers['Affine3'].db\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcIWHXLhncy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb3853a7-a5d8-4624-819e-b462c26b94db"
      },
      "source": [
        "\n",
        "lr = 0.01\n",
        "n_epochs = 10000\n",
        "batch_size = 128\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    batch_indexes = np.random.choice(x_train.shape[0], batch_size, replace=False)\n",
        "\n",
        "    batch_x = x_train[batch_indexes]\n",
        "    batch_y = y_train[batch_indexes]\n",
        "    \n",
        "    e = loss(batch_x, batch_y)\n",
        "    grad = gradient(layers, lastLayer)\n",
        "\n",
        "    params['W1'] -= lr * grad['W1']\n",
        "    params['b1'] -= lr * grad['b1']\n",
        "    params['W2'] -= lr * grad['W2']\n",
        "    params['b2'] -= lr * grad['b2']\n",
        "    params['W3'] -= lr * grad['W3']\n",
        "    params['b3'] -= lr * grad['b3']\n",
        "\n",
        "    #print(i, params['W1'])\n",
        "\n",
        "    if i % batch_size == 0:\n",
        "        print('loss=', e)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss= 15.020504960995275\n",
            "loss= 5.578668624901248\n",
            "loss= 4.378053236071812\n",
            "loss= 3.6090995676534434\n",
            "loss= 2.079817802810772\n",
            "loss= 1.7479190010645276\n",
            "loss= 1.7352224820739521\n",
            "loss= 1.7730512191609598\n",
            "loss= 1.201597949684475\n",
            "loss= 1.422297189070687\n",
            "loss= 1.2607803559639277\n",
            "loss= 1.2904797304525912\n",
            "loss= 1.1764473123204668\n",
            "loss= 1.431975066691565\n",
            "loss= 1.4544219356467178\n",
            "loss= 1.0179497200916376\n",
            "loss= 1.2283123827937872\n",
            "loss= 1.2007202623937303\n",
            "loss= 1.2346118500187413\n",
            "loss= 1.0818169455702447\n",
            "loss= 1.1064078099353292\n",
            "loss= 1.0045487181460082\n",
            "loss= 0.9678286585835492\n",
            "loss= 1.274069040253778\n",
            "loss= 1.2897279917217823\n",
            "loss= 0.9180152258859475\n",
            "loss= 1.05965151553072\n",
            "loss= 0.950134950246655\n",
            "loss= 1.1191918450790892\n",
            "loss= 0.8851342179970261\n",
            "loss= 0.6596525991156189\n",
            "loss= 1.0740833845693305\n",
            "loss= 0.8770699223205045\n",
            "loss= 0.7069342628134303\n",
            "loss= 0.8984189563996722\n",
            "loss= 0.851245888897949\n",
            "loss= 0.926321378569957\n",
            "loss= 0.8280299461195418\n",
            "loss= 0.9514537214214446\n",
            "loss= 1.0217478965814855\n",
            "loss= 0.8309359197616959\n",
            "loss= 1.0609195741606403\n",
            "loss= 0.8610046578077653\n",
            "loss= 0.9211332438237811\n",
            "loss= 1.0233834822723136\n",
            "loss= 0.6694923954666725\n",
            "loss= 0.9145289692312782\n",
            "loss= 0.6493677548258985\n",
            "loss= 0.7129377713233751\n",
            "loss= 1.122345835087386\n",
            "loss= 0.6894829604539636\n",
            "loss= 0.7335636204809504\n",
            "loss= 0.8394775753490791\n",
            "loss= 0.8248766606072357\n",
            "loss= 0.7217856119084198\n",
            "loss= 0.6309261817723326\n",
            "loss= 0.7535137520224202\n",
            "loss= 0.6452253719879275\n",
            "loss= 0.9830680816062565\n",
            "loss= 0.6409610844463605\n",
            "loss= 0.6730578809049745\n",
            "loss= 0.7219769193189736\n",
            "loss= 0.7822037948133633\n",
            "loss= 0.6411924747734843\n",
            "loss= 0.6524210506665065\n",
            "loss= 0.7279589461857395\n",
            "loss= 0.6325893212180411\n",
            "loss= 0.5937331568732728\n",
            "loss= 0.6374202786174274\n",
            "loss= 0.759192723965725\n",
            "loss= 0.6756664686022248\n",
            "loss= 0.7159324807402181\n",
            "loss= 0.6626230030731127\n",
            "loss= 0.45358401769967793\n",
            "loss= 0.707892834457112\n",
            "loss= 0.6720066069453046\n",
            "loss= 0.750843803311956\n",
            "loss= 0.667352058022991\n",
            "loss= 0.8528338467894376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwfX2cz4sojN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3409a969-2fd3-4606-a412-6df0059a8ed6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_mnist(x):\n",
        "    x = x.reshape(28, 28)\n",
        "    plt.imshow(x)\n",
        "    plt.gray()\n",
        "    plt.show()\n",
        "\n",
        "for index in range(10):\n",
        "    show_mnist(x_train[index])\n",
        "    output = predict(np.array([x_train[index]]))[0]\n",
        "    print(np.argmax(output))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMb0lEQVR4nO3db4gc9R3H8c8ntkWIotHQM9rUtMUnUmwsQQo9SoppiCIkfRKaByXS0vNBlQoVIlaoUgqhVouIFq5o/pTWUog2oZS2NkRtCZacksaoidqQYI54VxGpeZTqfftgJ3LG29lzZ2Znk+/7Bcfuznd35suQT+bf7vwcEQJw7lvQdgMABoOwA0kQdiAJwg4kQdiBJD4xyIXZ5tQ/0LCI8FzTK23Zba+xfdj267bvrDIvAM1yv9fZbZ8n6VVJ35B0XNI+SRsi4uWSz7BlBxrWxJb9OkmvR8SRiDgl6XeS1laYH4AGVQn7FZLemPX6eDHtQ2yP2Z6wPVFhWQAqavwEXUSMSxqX2I0H2lRlyz4paems158ppgEYQlXCvk/SVbY/Z/tTkr4laVc9bQGoW9+78RHxnu1bJf1F0nmSHouIl2rrDECt+r701tfCOGYHGtfIl2oAnD0IO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLvIZuBpt19992l9Xvvvbe0vmBB923ZypUrSz/7zDPPlNbPRpXCbvuopHclvS/pvYhYUUdTAOpXx5b96xHxVg3zAdAgjtmBJKqGPST91fbztsfmeoPtMdsTticqLgtABVV340cjYtL2pyU9ZftQRDw7+w0RMS5pXJJsR8XlAehTpS17REwWj9OSnpR0XR1NAahf32G3vdD2haefS1ot6WBdjQGoV5Xd+BFJT9o+PZ/fRsSfa+kKKdx8882l9U2bNpXWZ2Zm+l52RL4jyr7DHhFHJH2pxl4ANIhLb0AShB1IgrADSRB2IAnCDiTBT1zRmiuvvLK0fv755w+okxzYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAElxnR6NWrVrVtXbbbbdVmvehQ4dK6zfddFPX2tTUVKVln43YsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAElxnRyWjo6Ol9S1btnStXXTRRZWWfd9995XWjx07Vmn+5xq27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBNfZUcnGjRtL65dffnnf83766adL69u3b+973hn13LLbfsz2tO2Ds6ZdYvsp268Vj4uabRNAVfPZjd8qac0Z0+6UtDsirpK0u3gNYIj1DHtEPCvp7TMmr5W0rXi+TdK6mvsCULN+j9lHIuJE8fxNSSPd3mh7TNJYn8sBUJPKJ+giImxHSX1c0rgklb0PQLP6vfQ2ZXuJJBWP0/W1BKAJ/YZ9l6TT11w2StpZTzsAmuKI8j1r249LWilpsaQpST+W9AdJv5f0WUnHJK2PiDNP4s01L3bjzzKLFy8urfe6//rMzEzX2jvvvFP62fXr15fW9+zZU1rPKiI81/Sex+wRsaFL6fpKHQEYKL4uCyRB2IEkCDuQBGEHkiDsQBL8xDW5ZcuWldZ37NjR2LIfeuih0jqX1urFlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA6e3Jr1px5L9EPu+aaayrNf/fu3V1rDz74YKV54+Nhyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSfS8lXStC+NW0gO3bl35MHxbt24trS9cuLC0vnfv3tJ62e2ge92GGv3pditptuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kAS/Zz8HlN37vcn7vkvSkSNHSutcSx8ePbfsth+zPW374Kxp99ietL2/+Lux2TYBVDWf3fitkua6nckvImJ58fenetsCULeeYY+IZyW9PYBeADSoygm6W20fKHbzF3V7k+0x2xO2JyosC0BF/Yb9l5K+IGm5pBOS7u/2xogYj4gVEbGiz2UBqEFfYY+IqYh4PyJmJP1K0nX1tgWgbn2F3faSWS+/Kelgt/cCGA49r7PbflzSSkmLbR+X9GNJK20vlxSSjkq6pcEe0cOmTZu61mZmZhpd9ubNmxudP+rTM+wRsWGOyY820AuABvF1WSAJwg4kQdiBJAg7kARhB5LgJ65ngeXLl5fWV69e3diyd+7cWVo/fPhwY8tGvdiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASDNl8Fpieni6tL1rU9a5gPT333HOl9RtuuKG0fvLkyb6XjWYwZDOQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMHv2c8Cl156aWm9yu2iH3nkkdI619HPHWzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJrrMPgS1btpTWFyxo7v/kvXv3NjZvDJee/4psL7W9x/bLtl+y/YNi+iW2n7L9WvHY/x0UADRuPpuM9yT9MCKulvQVSd+3fbWkOyXtjoirJO0uXgMYUj3DHhEnIuKF4vm7kl6RdIWktZK2FW/bJmldU00CqO5jHbPbXibpWkn/lDQSESeK0puSRrp8ZkzSWP8tAqjDvM/82L5A0g5Jt0fEf2fXonPXyjlvJhkR4xGxIiJWVOoUQCXzCrvtT6oT9N9ExBPF5CnbS4r6Eknlt0AF0Kqeu/G2LelRSa9ExAOzSrskbZS0uXgsH9s3sV5DLq9ataq03usnrKdOnepae/jhh0s/OzU1VVrHuWM+x+xflfRtSS/a3l9Mu0udkP/e9nclHZO0vpkWAdShZ9gj4h+S5rzpvKTr620HQFP4uiyQBGEHkiDsQBKEHUiCsANJ8BPXAbj44otL65dddlml+U9OTnat3XHHHZXmjXMHW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Igt+zD8ChQ4dK672GTR4dHa2zHSTFlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHknBElL/BXippu6QRSSFpPCIetH2PpO9J+k/x1rsi4k895lW+MACVRcScoy7PJ+xLJC2JiBdsXyjpeUnr1BmP/WRE/Hy+TRB2oHndwj6f8dlPSDpRPH/X9iuSrqi3PQBN+1jH7LaXSbpW0j+LSbfaPmD7MduLunxmzPaE7YlKnQKopOdu/AdvtC+Q9Iykn0bEE7ZHJL2lznH8T9TZ1f9Oj3mwGw80rO9jdkmy/UlJf5T0l4h4YI76Mkl/jIgv9pgPYQca1i3sPXfjbVvSo5JemR304sTdad+UdLBqkwCaM5+z8aOS/i7pRUkzxeS7JG2QtFyd3fijkm4pTuaVzYstO9CwSrvxdSHsQPP63o0HcG4g7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJDHoIZvfknRs1uvFxbRhNKy9DWtfEr31q87eruxWGOjv2T+ycHsiIla01kCJYe1tWPuS6K1fg+qN3XggCcIOJNF22MdbXn6ZYe1tWPuS6K1fA+mt1WN2AIPT9pYdwIAQdiCJVsJue43tw7Zft31nGz10Y/uo7Rdt7297fLpiDL1p2wdnTbvE9lO2Xyse5xxjr6Xe7rE9Way7/bZvbKm3pbb32H7Z9ku2f1BMb3XdlfQ1kPU28GN22+dJelXSNyQdl7RP0oaIeHmgjXRh+6ikFRHR+hcwbH9N0klJ208PrWX7Z5LejojNxX+UiyJi05D0do8+5jDeDfXWbZjxm9Xiuqtz+PN+tLFlv07S6xFxJCJOSfqdpLUt9DH0IuJZSW+fMXmtpG3F823q/GMZuC69DYWIOBERLxTP35V0epjxVtddSV8D0UbYr5D0xqzXxzVc472HpL/aft72WNvNzGFk1jBbb0oaabOZOfQcxnuQzhhmfGjWXT/Dn1fFCbqPGo2IL0u6QdL3i93VoRSdY7Bhunb6S0lfUGcMwBOS7m+zmWKY8R2Sbo+I/86utbnu5uhrIOutjbBPSlo66/VnimlDISImi8dpSU+qc9gxTKZOj6BbPE633M8HImIqIt6PiBlJv1KL664YZnyHpN9ExBPF5NbX3Vx9DWq9tRH2fZKusv0525+S9C1Ju1ro4yNsLyxOnMj2QkmrNXxDUe+StLF4vlHSzhZ7+ZBhGca72zDjanndtT78eUQM/E/Sjeqckf+3pB+10UOXvj4v6V/F30tt9ybpcXV26/6nzrmN70q6VNJuSa9J+pukS4aot1+rM7T3AXWCtaSl3kbV2UU/IGl/8Xdj2+uupK+BrDe+LgskwQk6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji/5/q50l6GREBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAORUlEQVR4nO3dcaiVdZ7H8c8307Crha7u5dLE6o5BiVGK1NrG4jI4mUFq0DQm4brVHWLCMbZIZv/QWqKMHZcoGHDIxl1mkwHNZKgZy2TdrRi0cMvKGW9xQ+3qRSrGqdDt+t0/7nN379R9fud2nuc5z9Hv+wWXc87zPc95vpz6+Dzn+Z3z/MzdBeDcd17dDQBoDcIOBEHYgSAIOxAEYQeCOL+VGzMzTv0DFXN3G2l5oT27mS00s9+ZWY+ZrSnyWgCqZc2Os5vZGEm/l7RA0hFJeyUtc/d3E+uwZwcqVsWe/RpJPe7+gbuflrRF0uICrwegQkXCfomkw8MeH8mW/Qkz6zazfWa2r8C2ABRU+Qk6d98oaaPEYTxQpyJ79qOSLh32+FvZMgBtqEjY90q6zMymm9k4Sd+XtKOctgCUrenDeHf/0szulfQbSWMkbXL3d0rrDECpmh56a2pjfGYHKlfJl2oAnD0IOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCKLpKZsBSZo4cWKyPmHChNzaTTfdlFx36tSpyfqGDRuS9VOnTiXr0RQKu5n1SjopaUDSl+4+t4ymAJSvjD3737r7iRJeB0CF+MwOBFE07C5pp5m9YWbdIz3BzLrNbJ+Z7Su4LQAFFD2Mv97dj5rZn0t6ycwOuvue4U9w942SNkqSmXnB7QFoUqE9u7sfzW77JT0n6ZoymgJQvqbDbmYdZjZx6L6k70o6UFZjAMpV5DC+U9JzZjb0Ov/u7r8upSu0zLRp05L1Bx98MFmfN29esj5r1qxv2tKodXV1JeurVq2qbNtno6bD7u4fSLqqxF4AVIihNyAIwg4EQdiBIAg7EARhB4Iw99Z9qY1v0FXj8ssvz62tXr06ue7y5cuT9fHjxyfr2dBrrsOHD+fWTp48mVz3iiuuSNZPnEj//mr+/Pm5tYMHDybXPZu5+4j/UdizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQXEq6DVx88cXJ+vr165P12267LbfW6FLPRR06dChZv+GGG3JrY8eOTa7baCx8ypQpherRsGcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZ28DS5cuTdbvuuuuFnXyde+//36yvmDBgmQ99Xv2GTNmNNUTmsOeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJy9Ddx6662VvXZvb2+yvnfv3mS90ZTNqXH0RhpdFx7larhnN7NNZtZvZgeGLZtsZi+Z2aHsdlK1bQIoajSH8T+XtPAry9ZI2uXul0nalT0G0MYaht3d90j6+CuLF0vanN3fLGlJyX0BKFmzn9k73b0vu39MUmfeE82sW1J3k9sBUJLCJ+jc3VMTNrr7RkkbJSZ2BOrU7NDbcTPrkqTstr+8lgBUodmw75C0Iru/QtLz5bQDoCoND+PN7FlJ8yVNMbMjktZKekzSL83sTkkfSvpelU2e6+6+++5kvbs7fcpj586dubWenp7kuv399R2UdXbmnupBBRqG3d2X5ZS+U3IvACrE12WBIAg7EARhB4Ig7EAQhB0Igp+4toGPPvooWV+3bl1rGmmxefPm1d1CKOzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtmDW7VqVbLe0dFR2bavvPLKQuu/9tpryfrrr79e6PXPNezZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnPAhdeeGGyPnPmzNza2rVrk+suWrSoqZ6GnHdeen9x5syZpl+70e/8V65cmawPDAw0ve1zEXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYWGDt2bLI+e/bsZH3r1q3JeldXV27tiy++SK7baCy70W/CFy5cmKw3+o5Ayvnnp//3vOWWW5L1J554Ird2+vTppno6mzXcs5vZJjPrN7MDw5atM7OjZrY/+yv2zQwAlRvNYfzPJY30z/e/uPvV2d8L5bYFoGwNw+7ueyR93IJeAFSoyAm6e83srewwf1Lek8ys28z2mdm+AtsCUFCzYf+ppG9LulpSn6Sf5D3R3Te6+1x3n9vktgCUoKmwu/txdx9w9zOSfibpmnLbAlC2psJuZsPHepZKOpD3XADtwdw9/QSzZyXNlzRF0nFJa7PHV0tySb2SfuDufQ03Zpbe2Flq3LhxyXqjseht27YV2v5DDz2UW3vllVeS67766qvJ+uTJk5P1Rq8/a9asZL1Ky5cvz61t3749ue6pU6fKbqdl3N1GWt7wSzXuvmyExU8X7ghAS/F1WSAIwg4EQdiBIAg7EARhB4JoOPRW6sbO4qG31M9UH3744eS6DzzwQKFtv/jii8n6HXfckVv79NNPk+tOnTo1WX/hhfRvnObMmZOsp35K+vjjjyfXbTRst3jx4mQ95eWXX07W169fn6x/8sknTW9bkvbv319o/ZS8oTf27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsmTFjxiTrjzzySG7t/vvvT6772WefJetr1qxJ1rds2ZKsp8Z8585NXyDoqaeeStYbrd/T05Os33PPPbm13bt3J9e96KKLkvXrrrsuWU/9xPXmm29OrtvR0ZGsN3L48OFkffr06YVeP4VxdiA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2TGo8WJKefPLJ3Nrnn3+eXLe7uztZ37lzZ7J+7bXXJusrV67Mrd14443JdcePH5+sN/qt/jPPPJOsNxpvrsuyZSNdNPn/3X777YVe/7777kvWG30/oQjG2YHgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZM3196RmnU9dXbzS978GDB5P1Rr+dnjFjRrJexLp165L1Rx99NFkfGBgosRuUoelxdjO71Mx2m9m7ZvaOmf0oWz7ZzF4ys0PZ7aSymwZQntEcxn8p6R/cfaakv5L0QzObKWmNpF3ufpmkXdljAG2qYdjdvc/d38zun5T0nqRLJC2WtDl72mZJS6pqEkBx53+TJ5vZNEmzJf1WUqe7D33QPSapM2edbknpL4cDqNyoz8ab2QRJWyWtdvc/DK/54Fm+EU++uftGd5/r7ukrFwKo1KjCbmZjNRj0X7j7tmzxcTPryupdkvqraRFAGRoexpuZSXpa0nvuvmFYaYekFZIey26fr6TDFjl27Fiynhp6u+CCC5LrXnXVVU31NKTRtMl79uzJrW3fvj25bm9vb7LO0Nq5YzSf2f9a0h2S3jazoUmlf6zBkP/SzO6U9KGk71XTIoAyNAy7u/+XpBEH6SV9p9x2AFSFr8sCQRB2IAjCDgRB2IEgCDsQBD9xzUycODFZX7Ik/6v/c+bMSa7b35/+vtGmTZuS9dSUzJJ0+vTpZB2xcClpIDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcXbgHMM4OxAcYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTRMOxmdqmZ7Tazd83sHTP7UbZ8nZkdNbP92d+i6tsF0KyGF68wsy5JXe7+pplNlPSGpCUanI/9j+7+z6PeGBevACqXd/GK0czP3iepL7t/0szek3RJue0BqNo3+sxuZtMkzZb022zRvWb2lpltMrNJOet0m9k+M9tXqFMAhYz6GnRmNkHSf0h6xN23mVmnpBOSXNI/afBQ/+8bvAaH8UDF8g7jRxV2Mxsr6VeSfuPuG0aoT5P0K3ef1eB1CDtQsaYvOGlmJulpSe8ND3p24m7IUkkHijYJoDqjORt/vaT/lPS2pDPZ4h9LWibpag0exvdK+kF2Mi/1WuzZgYoVOowvC2EHqsd144HgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0E0vOBkyU5I+nDY4ynZsnbUrr21a18SvTWrzN7+Iq/Q0t+zf23jZvvcfW5tDSS0a2/t2pdEb81qVW8cxgNBEHYgiLrDvrHm7ae0a2/t2pdEb81qSW+1fmYH0Dp179kBtAhhB4KoJexmttDMfmdmPWa2po4e8phZr5m9nU1DXev8dNkcev1mdmDYsslm9pKZHcpuR5xjr6be2mIa78Q047W+d3VPf97yz+xmNkbS7yUtkHRE0l5Jy9z93ZY2ksPMeiXNdffav4BhZn8j6Y+S/nVoai0ze1zSx+7+WPYP5SR3f7BNelunbziNd0W95U0z/neq8b0rc/rzZtSxZ79GUo+7f+DupyVtkbS4hj7anrvvkfTxVxYvlrQ5u79Zg/+ztFxOb23B3fvc/c3s/klJQ9OM1/reJfpqiTrCfomkw8MeH1F7zffuknaa2Rtm1l13MyPoHDbN1jFJnXU2M4KG03i30lemGW+b966Z6c+L4gTd113v7nMk3Sjph9nhalvywc9g7TR2+lNJ39bgHIB9kn5SZzPZNONbJa129z8Mr9X53o3QV0vetzrCflTSpcMefytb1hbc/Wh22y/pOQ1+7Ggnx4dm0M1u+2vu5/+4+3F3H3D3M5J+phrfu2ya8a2SfuHu27LFtb93I/XVqvetjrDvlXSZmU03s3GSvi9pRw19fI2ZdWQnTmRmHZK+q/abinqHpBXZ/RWSnq+xlz/RLtN4500zrprfu9qnP3f3lv9JWqTBM/LvS/rHOnrI6esvJf139vdO3b1JelaDh3X/o8FzG3dK+jNJuyQdkvSypMlt1Nu/aXBq77c0GKyumnq7XoOH6G9J2p/9Lar7vUv01ZL3ja/LAkFwgg4IgrADQRB2IAjCDgRB2IEgCDsQBGEHgvhfT0hvXT6gH6cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqtJcgWK71d",
        "colab_type": "text"
      },
      "source": [
        "これまでに見てきたように、どんなに複雑なネットワークでも、各層ごとに forward と backward を正確に定義すれば、Backpropagationを使って勾配を簡単に求めることができます。\n",
        "勾配の計算は各層ごとに閉じており、局所的な計算だけで全体の勾配を求めることができるためとても便利です。"
      ]
    }
  ]
}